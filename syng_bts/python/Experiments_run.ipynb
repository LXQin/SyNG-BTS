{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ syng-bts==2.1\n",
    "# if you need to reinstall, use this command\n",
    "# %pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --upgrade --no-deps --force-reinstall syng-bts==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syng_bts.python.Experiments_new import *\n",
    "# from Experiments_new import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run pilot experiments\n",
    "# This function trains VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \n",
    "#      with several pilot size given data, model, batch_size, learning_rate, epoch, off_aug and pre_model\n",
    "#      for each pilot size, there will be 5 draws, \n",
    "#      for each draw, the data is augmented to 5 times the original sample size.\n",
    "# dataname :         pure data name without .csv. Eg: SKCMPositive_3\n",
    "# pilot_size:        a set including potential pilot sizes\n",
    "# model:             name of the model to be trained\n",
    "# batch_frac:        batch fraction\n",
    "# learning_rate:     learning rate \n",
    "# epoch:             choose from None (early_stop), or any interger, if choose None, early_stop_num will take effect\n",
    "# early_stop_num:    if loss does not improve for early_stop_num epochs, the training will stop. Default value is 30. Only take effect when epoch == \"early_stop\"\n",
    "# off_aug:           choose from AE_head, Gaussian_head, None. if choose AE_head, AE_head_num will take effect. If choose Gaussian_head, Gaussian_head_num will take effect. If choose None, no offline augmentation\n",
    "# AE_head_num:       how many folds of AEhead augmentation needed. Default value is 2, Only take effect when off_aug == \"AE_head\"\n",
    "# Gaussian_head_num: how many folds of Gaussianhead augmentation needed. Default value is 9, Only take effect when off_aug == \"Gaussian_head\"\n",
    "# pre_model:         transfer learning input model. If pre_model == None, no transfer learning\n",
    "\n",
    "PilotExperiment(dataname = \"SKCMPositive_4\", pilot_size = [100],\n",
    "                model = \"VAE1-10\", batch_frac = 0.1, \n",
    "                learning_rate = 0.0005, pre_model = None,\n",
    "                epoch = None,  off_aug = None, early_stop_num = 30,\n",
    "                AE_head_num = 2, Gaussian_head_num = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% running application on case study BRCASubtype\n",
    "# This function trains VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \n",
    "#      given data, model, batch_size, learning_rate, epoch, off_aug and pre_model\n",
    "#      and generate new samples with size specified by the users.\n",
    "# path:              path for reading real data and saving new data\n",
    "# dataname :         pure data name without .csv. Eg: SKCMPositive_3\n",
    "# apply_log:         logical whether apply log transformation before training\n",
    "# model:             name of the model to be trained\n",
    "# batch_frac:        batch fraction\n",
    "# learning_rate:     learning rate \n",
    "# epoch:             choose from None (early_stop), or any interger, if choose None, early_stop_num will take effect\n",
    "# early_stop_num:    if loss does not improve for early_stop_num epochs, the training will stop. Default value is 30. Only take effect when epoch == \"early_stop\"\n",
    "# off_aug:           choose from AE_head, Gaussian_head, None. if choose AE_head, AE_head_num will take effect. If choose Gaussian_head, Gaussian_head_num will take effect. If choose None, no offline augmentation\n",
    "# AE_head_num:       how many folds of AEhead augmentation needed. Default value is 2, Only take effect when off_aug == \"AE_head\"\n",
    "# Gaussian_head_num: how many folds of Gaussianhead augmentation needed. Default value is 9, Only take effect when off_aug == \"Gaussian_head\"\n",
    "# pre_model:         transfer learning input model. If pre_model == None, no transfer learning\n",
    "# save_model:        if the trained model should be saved, specify the path and name of the saved model\n",
    "        \n",
    "ApplyExperiment(path = \"../Case/BRCASubtype/\", dataname = \"BRCASubtypeSel\", apply_log = True, \n",
    "                new_size = [1000], model = \"CVAE1-20\" , batch_frac = 0.1, \n",
    "                learning_rate = 0.0005, epoch = 10, early_stop_num = 30, \n",
    "                off_aug = None, AE_head_num = 2, Gaussian_head_num = 9, \n",
    "                pre_model = None, save_model = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Running transfer learning\n",
    "# This function run transfer learning using VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \n",
    "#      given fromdata, todata, model, batch_size, learning_rate, epoch, off_aug and pre_model\n",
    "#      and generate new samples with size specified by the users.\n",
    "#      The fine tuning model training can be pilot experiments or apply experiment\n",
    "# Make sure data files for pre_model training and fine tuning model training are in Transfer/\n",
    "# pilot_size:        if None, the fine tuning model will be apply experiment and new_size will take effect\n",
    "#                    otherwise, the fine tuning model will be trained using pilot experiments\n",
    "# fromname:          the dataname for pre_model training \n",
    "# toname:            the dataname for fine tuning model training\n",
    "# fromsize:          the sample size of the fromdata\n",
    "# new_size:          if apply experiment, this will be the sample size of generated samples\n",
    "# apply_log:         logical whether apply log transformation before training\n",
    "# model:             name of the model to be trained\n",
    "# batch_frac:        batch fraction\n",
    "# learning_rate:     learning rate \n",
    "# epoch:             choose from None (early_stop), or any interger, if choose None, early_stop_num will take effect\n",
    "# off_aug:           choose from AE_head, Gaussian_head, None. if choose AE_head, AE_head_num will take effect. If choose Gaussian_head, Gaussian_head_num will take effect. If choose None, no offline augmentation\n",
    "TransferExperiment(pilot_size = None, fromname = \"PRAD\", toname = \"BRCA\", fromsize = 551, \n",
    "         new_size = 500, apply_log = True, model = \"maf\", epoch = 10,\n",
    "         batch_frac = 0.1, learning_rate = 0.0005, off_aug = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
